{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Steps :\n",
    "#1: Grab The Data\n",
    "#2: Separate the Data into Dependant and Independent Variables\n",
    "#3: Deal with Missing values (Impute/Drop) ** Not Required in this case **\n",
    "#4: Encode Non Numeric categorical Data and create dummy variables\n",
    "#5: Eliminate the Non Influencing factors (independent variables) \n",
    "#6: Feature Scale ** Not Used in this case **\n",
    "#7: Apply Dimensionality Reduction ** Not Used in this case **\n",
    "#8: Divide the data into training and testing data\n",
    "#9: Build the Model, Check the Co-Efficient and the Intercepts\n",
    "#10: Run The Model on Test Data using K-Fold Cross Val \n",
    "#11: Tune the Hyperparameters of the algorithm and Go to Step 9 till you find satisfactory accuracy\n",
    "#12: Interpret the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "dataset.head() ## Looking at top 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[165349.2 136897.8 471784.1 'New York']\n",
      " [162597.7 151377.59 443898.53 'California']\n",
      " [153441.51 101145.55 407934.54 'Florida']\n",
      " [144372.41 118671.85 383199.62 'New York']\n",
      " [142107.34 91391.77 366168.42 'Florida']]\n",
      "\n",
      "\n",
      "[192261.83 191792.06 191050.39 182901.99 166187.94]\n"
     ]
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit, Independent Variable\n",
    "y = dataset.iloc[:, 4].values  ## Grab Profit, which is dependant variable as it is dependant on R&D, Admin, Marketing, State etc\n",
    "\n",
    "print(X[0:5]) ## print First 5 rows\n",
    "print(\"\\n\")\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New York' 'California' 'Florida']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "## Lets see how many unique states exist\n",
    "unique=dataset.State.unique()\n",
    "print(unique)\n",
    "\n",
    "count=dataset.State.unique().size\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After encoding: [[165349.2 136897.8 471784.1 2]\n",
      " [162597.7 151377.59 443898.53 0]\n",
      " [153441.51 101145.55 407934.54 1]\n",
      " [144372.41 118671.85 383199.62 2]\n",
      " [142107.34 91391.77 366168.42 1]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical data : State in this case\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3]) ## X is encoded, '3' indicates that 3rd column to be encoded\n",
    "\n",
    "print(\"After encoding:\",X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'categorical_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ffaae7d788bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0monehotencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorical_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## column of the categorical column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0monehotencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## to create dummy variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## Dummy variables appear as first columns followed by other columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'categorical_features'"
     ]
    }
   ],
   "source": [
    "onehotencoder = OneHotEncoder(categorical_features = [3]) ## column of the categorical column\n",
    "X = onehotencoder.fit_transform(X).toarray() ## to create dummy variables\n",
    "print(X[0:5]) ## Dummy variables appear as first columns followed by other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[136897.8 471784.1 2]\n",
      " [151377.59 443898.53 0]\n",
      " [101145.55 407934.54 1]\n",
      " [118671.85 383199.62 2]\n",
      " [91391.77 366168.42 1]]\n"
     ]
    }
   ],
   "source": [
    "# Avoiding the Dummy Variable Trap\n",
    "X = X[:, 1:] ## Remove the first column of X, ie, at index 0\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling (Multiple Linear Regression libraries do it automatically)\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc_X = StandardScaler()\n",
    "#X_train = sc_X.fit_transform(X_train)\n",
    "#X_test = sc_X.transform(X_test)\n",
    "#sc_y = StandardScaler()\n",
    "#y_train = sc_y.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   0.00000000e+00   5.54939500e+04   1.03057490e+05\n",
      "    2.14634810e+05]\n",
      " [  0.00000000e+00   1.00000000e+00   4.60140200e+04   8.50474400e+04\n",
      "    2.05517640e+05]\n",
      " [  1.00000000e+00   0.00000000e+00   7.53288700e+04   1.44135980e+05\n",
      "    1.34050070e+05]\n",
      " [  0.00000000e+00   0.00000000e+00   4.64260700e+04   1.57693920e+05\n",
      "    2.10797670e+05]\n",
      " [  1.00000000e+00   0.00000000e+00   9.17491600e+04   1.14175790e+05\n",
      "    2.94919570e+05]]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(X_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 42554.1676177\n",
      "Regression Coeff: [ -9.59284160e+02   6.99369053e+02   7.73467193e-01   3.28845975e-02\n",
      "   3.66100259e-02]\n"
     ]
    }
   ],
   "source": [
    "# Fitting Multiple Linear Regression to the Training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Intercept ## Mean value of Y when X=0 [constant=when all independent variables are zero]\n",
    "print(\"Intercept:\",regressor.intercept_)\n",
    "\n",
    "# Co-Efficient of each Variable\n",
    "print(\"Regression Coeff:\",regressor.coef_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 103015.20159796  132582.27760815  132447.73845175   71976.09851258\n",
      "  178537.48221056  116161.24230166   67851.69209676   98791.73374687\n",
      "  113969.43533013  167921.06569551]\n",
      "\n",
      "\n",
      "[ 103282.38  144259.4   146121.95   77798.83  191050.39  105008.31\n",
      "   81229.06   97483.56  110352.25  166187.94]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(y_pred)\n",
    "print(\"\\n\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 7514.29365964\n",
      "MSE: 83502864.0326\n",
      "RMSE: 9137.99015279\n",
      "Explained Variance Score: 94.6919285865\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "## Higher the Explained Variance Score, the better the model is:\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_test, y_pred)*100)\n",
    "## 94.6% of the variance or variability of the data is explained by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.934706847328\n",
      "adj_r2= 0.853090406489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score \n",
    "print('r2:',r2_score(y_test, y_pred)) ## Closer to 1 means better prediction\n",
    "\n",
    "adj_r2=1 - float(len(y_test)-1)/(len(y_test)-len(regressor.coef_)-1)*(1 - metrics.r2_score(y_test,y_pred))\n",
    "print(\"adj_r2=\",adj_r2) ##Closer to 1 the better the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='neg_median_absolute_error') \n",
    "print (regression_avg.mean())\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='explained_variance') \n",
    "print (regression_avg.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting against Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv('Startups_Test_Samp.csv')\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_samp = dataset_test.iloc[:, :-1].values ## Grab everything except Profit\n",
    "y_test_samp = dataset_test.iloc[:, 4].values  ## Grab Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "X_test_samp[:, 3] = labelencoder.fit_transform(X_test_samp[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X_test_samp = onehotencoder.fit_transform(X_test_samp).toarray()\n",
    "\n",
    "# Avoiding the Dummy Variable Trap\n",
    "X_test_samp = X_test_samp[:, 1:]\n",
    "X_test_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = regressor.predict(X_test_samp)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Optimal Model using Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Multiple Linear Regression : y=b0+b1x1+b2x2+......+bnxn\n",
    "## Add x0 which is linked with variable b0 and always equals to 1.\n",
    "#X=np.append(arr=X,values=np.ones((50,1)).astype(int), axis=1) ## Add 1; 50 times; axis=1 as it is a row\n",
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit\n",
    "y = dataset.iloc[:, 4].values  ## Grab Profit\n",
    "\n",
    "\n",
    "# Encoding categorical data : Country in this case\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray() ## to remove relationship\n",
    "\n",
    "# Avoiding the Dummy Variable Trap\n",
    "X = X[:, 1:] ## Remove the first column of X, ie, at index 0\n",
    "X[0:5]\n",
    "\n",
    "## y=bo+b1x1+bnxn can also be written as :\n",
    "## y=boxo+b1x1+bnxn [assuming x0=1]\n",
    "X=np.append(arr=np.ones((50,1)).astype(int),values=X, axis=1) ## axis=1 means add columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.head())\n",
    "print(\"\\n\")\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_Opt=X[:,[0,1,2,3,4,5]]\n",
    "#X_Opt=X[:,]\n",
    "#X_Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor_OLS=sm.OLS(endog=y,exog=X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## Significance Level = 0.05 or 5%\n",
    "## Lower the P-Value , better it is\n",
    "## Lower P Value, reject the null hypothesuis, accept alternate hypothesis.\n",
    "## Alternate hypothesis= This particular variable is influencing the outcome\n",
    "## Find out the independent variable with highest P-Value\n",
    "## If the variable with highest P-value is greater than Significance level, drop that variable\n",
    "## Perform this exericse till the indepedent variable with highest P-Value is lower than significance level\n",
    "## x1=New York\n",
    "##x2=California\n",
    "## x3=R&D Spend\n",
    "## x4=Admin Spend\n",
    "##x5=Marketing Spend\n",
    "regressor_OLS.summary()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[:,[0,1,3,4,5]] ## REmoved x2=California, highest P-value\n",
    "X[0]\n",
    " \n",
    "\n",
    "##x1=New York\n",
    "## x2=R&D Spend\n",
    "## x3=Admin Spend\n",
    "##x4=Marketing Spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "regressor_OLS=sm.OLS(endog=y,exog=X).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[:,[0,2,3,4]] ## REmoved x1=New York, highest P-value.\n",
    "regressor_OLS=sm.OLS(endog=y,exog=X).fit()\n",
    "regressor_OLS.summary() \n",
    "\n",
    "## x1=R&D Spend\n",
    "## x2=Admin Spend\n",
    "## x3=Marketing Spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[:,[0,1,3]] ## Removed x2=Admin Spend, highest P-value; (3 and 5: R&D Spend and Marketing Spend)\n",
    "regressor_OLS=sm.OLS(endog=y,exog=X).fit()\n",
    "regressor_OLS.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## x1=R&D Spend\n",
    "## x2=Marketing Spend\n",
    "X=X[:,[0,1]] ## Removed x2=Marketing Spend, highest P-value; (Left Over; x2: R&D Spend )\n",
    "regressor_OLS=sm.OLS(endog=y,exog=X).fit()\n",
    "regressor_OLS.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.head())\n",
    "print(\"\\n\")\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0:5])\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(y_pred)\n",
    "print(\"\\n\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "## Higher the Explained Variance Score, the better the model is:\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_test, y_pred)*100)\n",
    "## 95.9% of the variance is explained by the model\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='neg_mean_absolute_error') \n",
    "print (\"Cross val Mean Abs Error:\",regression_avg.mean())\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='explained_variance') \n",
    "print (\"Cross Val Explained Variance:\",regression_avg.mean()*100)\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score \n",
    "print('r2:',r2_score(y_test, y_pred)) ## Closer to 1 means better prediction\n",
    "\n",
    "adj_r2=1 - float(len(y_test)-1)/(len(y_test)-len(regressor.coef_)-1)*(1 - metrics.r2_score(y_test,y_pred))\n",
    "print(\"adj_r2=\",adj_r2) ##Closer to 1 the better the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Including Marketing Spend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting the model with training data of significant independent variable\n",
    "print(dataset.head())\n",
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit\n",
    "X=X[:,[0,2]] ## Grabbing R&D Spend and Marketing Spend\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "## Higher the Explained Variance Score, the better the model is:\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_test, y_pred)*100)\n",
    "## 95.9% of the variance is explained by the model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='neg_mean_absolute_error') \n",
    "print (\"Cross Val Mean Absolute Error\",regression_avg.mean())\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='explained_variance') \n",
    "print (\"Cross Val Explained Variance:\",regression_avg.mean()*100)\n",
    "\n",
    "from sklearn.metrics import r2_score \n",
    "print('r2:',r2_score(y_test, y_pred)) ## Closer to 1 means better prediction\n",
    "\n",
    "adj_r2=1 - float(len(y_test)-1)/(len(y_test)-len(regressor.coef_)-1)*(1 - metrics.r2_score(y_test,y_pred))\n",
    "print(\"adj_r2=\",adj_r2) ##Closer to 1 the better the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to automate Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit\n",
    "y = dataset.iloc[:, 4].values  ## Grab Profit\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple Linear Regression : y=b0+b1x1+b2x2+......+bnxn\n",
    "## Add x0 which is linked with variable b0 and always equals to 1.\n",
    "#X=np.append(arr=X,values=np.ones((50,1)).astype(int), axis=1) ## Add 1; 50 times; axis=1 as it is a row\n",
    "\n",
    "# Encoding categorical data : Country in this case\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray() ## to remove relationship\n",
    "\n",
    "# Avoiding the Dummy Variable Trap\n",
    "X = X[:, 1:] ## Remove the first column of X, ie, at index 0\n",
    "X[0:5]\n",
    "\n",
    "X=np.append(arr=np.ones((50,1)).astype(int),values=X, axis=1)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        import statsmodels.formula.api as sm\n",
    "        def backwardElimination(x, sl):\n",
    "            numVars = len(x[0])\n",
    "            for i in range(0, numVars):\n",
    "                regressor_OLS = sm.OLS(y, x).fit()\n",
    "                maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "                if maxVar > sl:\n",
    "                    for j in range(0, numVars - i):\n",
    "                        if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                            x = np.delete(x, j, 1)\n",
    "            regressor_OLS.summary()\n",
    "            return x\n",
    "         \n",
    "        SL = 0.05\n",
    "        X = backwardElimination(X, SL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.head())\n",
    "print(\"\\n\")\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "regressor_OLS=sm.OLS(endog=y_train,exog=X_train).fit()\n",
    "y_pred = regressor_OLS.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "## Higher the Explained Variance Score, the better the model is:\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_test, y_pred)*100)\n",
    "## 95.9% of the variance is explained by the model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='neg_mean_absolute_error') \n",
    "print (\"Cross Val Mean Absolute Error\",regression_avg.mean())\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='explained_variance') \n",
    "print (\"Cross Val Explained Variance:\",regression_avg.mean()*100)\n",
    "\n",
    "from sklearn.metrics import r2_score \n",
    "print('r2:',r2_score(y_test, y_pred)) ## Closer to 1 means better prediction\n",
    "\n",
    "adj_r2=1 - float(len(y_test)-1)/(len(y_test)-len(regressor.coef_)-1)*(1 - metrics.r2_score(y_test,y_pred))\n",
    "print(\"adj_r2=\",adj_r2) ##Closer to 1 the better the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Automate Backward Elimination with R-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit\n",
    "y = dataset.iloc[:, 4].values  ## Grab Profit\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple Linear Regression : y=b0+b1x1+b2x2+......+bnxn\n",
    "## Add x0 which is linked with variable b0 and always equals to 1.\n",
    "#X=np.append(arr=X,values=np.ones((50,1)).astype(int), axis=1) ## Add 1; 50 times; axis=1 as it is a row\n",
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit\n",
    "y = dataset.iloc[:, 4].values  ## Grab Profit\n",
    "\n",
    "\n",
    "# Encoding categorical data : Country in this case\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray() ## to remove relationship\n",
    "\n",
    "# Avoiding the Dummy Variable Trap\n",
    "X = X[:, 1:] ## Remove the first column of X, ie, at index 0\n",
    "X[0:5]\n",
    "\n",
    "X=np.append(arr=np.ones((50,1)).astype(int),values=X, axis=1)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## Find the model using elimination + highest adjusted_r2    \n",
    "    import statsmodels.formula.api as sm\n",
    "        def backwardElimination(x, SL):\n",
    "            numVars = len(x[0])\n",
    "            temp = np.zeros((50,6)).astype(int) ## Only change this line\n",
    "            for i in range(0, numVars):\n",
    "                regressor_OLS = sm.OLS(y, x).fit()\n",
    "                maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "                adjR_before = regressor_OLS.rsquared_adj.astype(float)\n",
    "                if maxVar > SL:\n",
    "                    for j in range(0, numVars - i):\n",
    "                        if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                            temp[:,j] = x[:, j]\n",
    "                            x = np.delete(x, j, 1)\n",
    "                            tmp_regressor = sm.OLS(y, x).fit()\n",
    "                            adjR_after = tmp_regressor.rsquared_adj.astype(float)\n",
    "                            if (adjR_before >= adjR_after):\n",
    "                                x_rollback = np.hstack((x, temp[:,[0,j]]))\n",
    "                                x_rollback = np.delete(x_rollback, j, 1)\n",
    "                                print (regressor_OLS.summary())\n",
    "                                return x_rollback\n",
    "                            else:\n",
    "                                continue\n",
    "            regressor_OLS.summary()\n",
    "            return x\n",
    "         \n",
    "        SL = 0.05\n",
    "        X = backwardElimination(X , SL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.head())\n",
    "print(\"\\n\")\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "regressor_OLS=sm.OLS(endog=y_train,exog=X_train).fit()\n",
    "y_pred = regressor_OLS.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "## Higher the Explained Variance Score, the better the model is:\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_test, y_pred)*100)\n",
    "## 95.9% of the variance is explained by the model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='neg_mean_absolute_error') \n",
    "print (\"Cross Val Mean Absolute Error\",regression_avg.mean())\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='explained_variance') \n",
    "print (\"Cross Val Explained Variance:\",regression_avg.mean()*100)\n",
    "\n",
    "from sklearn.metrics import r2_score \n",
    "print('r2:',r2_score(y_test, y_pred)) ## Closer to 1 means better prediction\n",
    "\n",
    "adj_r2=1 - float(len(y_test)-1)/(len(y_test)-len(regressor.coef_)-1)*(1 - metrics.r2_score(y_test,y_pred))\n",
    "print(\"adj_r2=\",adj_r2) ##Closer to 1 the better the prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
