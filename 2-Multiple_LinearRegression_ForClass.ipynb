{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Steps :\n",
    "#1: Grab The Data\n",
    "#2: Separate the Data into Dependant and Independent Variables\n",
    "#3: Deal with Missing values (Impute/Drop) ** Not Required in this case **\n",
    "#4: Encode Non Numeric categorical Data and create dummy variables\n",
    "#5: Eliminate the Non Influencing factors (independent variables) \n",
    "#6: Feature Scale ** Not Used in this case **\n",
    "#7: Apply Dimensionality Reduction ** Not Used in this case **\n",
    "#8: Divide the data into training and testing data\n",
    "#9: Build the Model, Check the Co-Efficient and the Intercepts\n",
    "#10: Run The Model on Test Data using K-Fold Cross Val \n",
    "#11: Tune the Hyperparameters of the algorithm and Go to Step 9 till you find satisfactory accuracy\n",
    "#12: Interpret the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "dataset.head() ## Looking at top 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[165349.2 136897.8 471784.1 'New York']\n",
      " [162597.7 151377.59 443898.53 'California']\n",
      " [153441.51 101145.55 407934.54 'Florida']\n",
      " [144372.41 118671.85 383199.62 'New York']\n",
      " [142107.34 91391.77 366168.42 'Florida']]\n",
      "\n",
      "\n",
      "[192261.83 191792.06 191050.39 182901.99 166187.94]\n"
     ]
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit, Independent Variable\n",
    "y = dataset.iloc[:, 4].values  ## Grab Profit, which is dependant variable as it is dependant on R&D, Admin, Marketing, State etc\n",
    "\n",
    "print(X[0:5]) ## print First 5 rows\n",
    "print(\"\\n\")\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New York' 'California' 'Florida']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "## Lets see how many unique states exist\n",
    "unique=dataset.State.unique()\n",
    "print(unique)\n",
    "\n",
    "count=dataset.State.unique().size\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After encoding: [[165349.2 136897.8 471784.1 2]\n",
      " [162597.7 151377.59 443898.53 0]\n",
      " [153441.51 101145.55 407934.54 1]\n",
      " [144372.41 118671.85 383199.62 2]\n",
      " [142107.34 91391.77 366168.42 1]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical data : State in this case\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3]) ## X is encoded, '3' indicates that 3rd column to be encoded\n",
    "\n",
    "print(\"After encoding:\",X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 0.0000000e+00 1.0000000e+00 1.6534920e+05 1.3689780e+05\n",
      "  4.7178410e+05]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6259770e+05 1.5137759e+05\n",
      "  4.4389853e+05]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5344151e+05 1.0114555e+05\n",
      "  4.0793454e+05]\n",
      " [0.0000000e+00 0.0000000e+00 1.0000000e+00 1.4437241e+05 1.1867185e+05\n",
      "  3.8319962e+05]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4210734e+05 9.1391770e+04\n",
      "  3.6616842e+05]]\n"
     ]
    }
   ],
   "source": [
    "onehotencoder = OneHotEncoder(categorical_features = [3]) ## column of the categorical column\n",
    "X = onehotencoder.fit_transform(X).toarray() ## to create dummy variables\n",
    "print(X[0:5]) ## Dummy variables appear as first columns followed by other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 1.0000000e+00 1.6534920e+05 1.3689780e+05 4.7178410e+05]\n",
      " [0.0000000e+00 0.0000000e+00 1.6259770e+05 1.5137759e+05 4.4389853e+05]\n",
      " [1.0000000e+00 0.0000000e+00 1.5344151e+05 1.0114555e+05 4.0793454e+05]\n",
      " [0.0000000e+00 1.0000000e+00 1.4437241e+05 1.1867185e+05 3.8319962e+05]\n",
      " [1.0000000e+00 0.0000000e+00 1.4210734e+05 9.1391770e+04 3.6616842e+05]]\n"
     ]
    }
   ],
   "source": [
    "# Avoiding the Dummy Variable Trap\n",
    "X = X[:, 1:] ## Remove the first column of X, ie, at index 0\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling (Multiple Linear Regression libraries do it automatically)\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc_X = StandardScaler()\n",
    "#X_train = sc_X.fit_transform(X_train)\n",
    "#X_test = sc_X.transform(X_test)\n",
    "#sc_y = StandardScaler()\n",
    "#y_train = sc_y.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 0.0000000e+00 5.5493950e+04 1.0305749e+05 2.1463481e+05]\n",
      " [0.0000000e+00 1.0000000e+00 4.6014020e+04 8.5047440e+04 2.0551764e+05]\n",
      " [1.0000000e+00 0.0000000e+00 7.5328870e+04 1.4413598e+05 1.3405007e+05]\n",
      " [0.0000000e+00 0.0000000e+00 4.6426070e+04 1.5769392e+05 2.1079767e+05]\n",
      " [1.0000000e+00 0.0000000e+00 9.1749160e+04 1.1417579e+05 2.9491957e+05]]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(X_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF Train: 20.074179210843095\n",
      "VIF Test: 15.315541662189968\n",
      "\n",
      "\n",
      "Intercept: 42554.16761772438\n",
      "Regression Coeff: [-9.59284160e+02  6.99369053e+02  7.73467193e-01  3.28845975e-02\n",
      "  3.66100259e-02]\n"
     ]
    }
   ],
   "source": [
    "# Fitting Multiple Linear Regression to the Training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "## Calculating VIF Score\n",
    "r2_mc=regressor.score(X_train,y_train)\n",
    "r2_mc_test=regressor.score(X_test,y_test)\n",
    "vif_train=1/(1-r2_mc) ## to determine multi collinearity. Greater than 5 means very high multi collinearity\n",
    "print(\"VIF Train:\",vif_train)\n",
    "\n",
    "vif_test=1/(1-r2_mc_test) ## to determine multi collinearity. Greater than 5 means very high multi collinearity\n",
    "print(\"VIF Test:\",vif_test)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Intercept ## Mean value of Y when X=0 [constant=when all independent variables are zero]\n",
    "print(\"Intercept:\",regressor.intercept_)\n",
    "\n",
    "# Co-Efficient of each Variable\n",
    "print(\"Regression Coeff:\",regressor.coef_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103015.20159796 132582.27760815 132447.73845175  71976.09851258\n",
      " 178537.48221056 116161.24230166  67851.69209676  98791.73374687\n",
      " 113969.43533013 167921.06569551]\n",
      "\n",
      "\n",
      "[103282.38 144259.4  146121.95  77798.83 191050.39 105008.31  81229.06\n",
      "  97483.56 110352.25 166187.94]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(y_pred)\n",
    "print(\"\\n\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 7514.293659640891\n",
      "MSE: 83502864.03257468\n",
      "RMSE: 9137.990152794797\n",
      "Explained Variance Score: 94.69192858652667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "## Higher the Explained Variance Score, the better the model is:\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_test, y_pred)*100)\n",
    "## 94.6% of the variance or variability of the data is explained by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.9347068473282446\n",
      "adj_r2= 0.8530904064885504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score \n",
    "print('r2:',r2_score(y_test, y_pred)) \n",
    "## Closer to 1 means better prediction. r2 indicates the closeness to regression line to actual line\n",
    "\n",
    "adj_r2=1 - float(len(y_test)-1)/(len(y_test)-len(regressor.coef_)-1)*(1 - metrics.r2_score(y_test,y_pred))\n",
    "print(\"adj_r2=\",adj_r2) \n",
    "##Closer to 1 the better the prediction. However, it punishes if unnecessary variables are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7064.173171667285\n",
      "76.03431352407961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='neg_median_absolute_error') \n",
    "print (regression_avg.mean())\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='explained_variance') \n",
    "print (regression_avg.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting against Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185349.20</td>\n",
       "      <td>146897.80</td>\n",
       "      <td>571784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>182597.70</td>\n",
       "      <td>161377.59</td>\n",
       "      <td>343898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154372.41</td>\n",
       "      <td>128671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132107.34</td>\n",
       "      <td>81391.77</td>\n",
       "      <td>466168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State  Profit\n",
       "0  185349.20       146897.80        571784.10    New York     NaN\n",
       "1  182597.70       161377.59        343898.53  California     NaN\n",
       "2  153441.51       101145.55        407934.54     Florida     NaN\n",
       "3  154372.41       128671.85        383199.62    New York     NaN\n",
       "4  132107.34        81391.77        466168.42     Florida     NaN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = pd.read_csv('Startups_Test_Samp.csv')\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_samp = dataset_test.iloc[:, :-1].values ## Grab everything except Profit\n",
    "y_test_samp = dataset_test.iloc[:, 4].values  ## Grab Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 1.0000000e+00, 1.8534920e+05, 1.4689780e+05,\n",
       "        5.7178410e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.8259770e+05, 1.6137759e+05,\n",
       "        3.4389853e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.5344151e+05, 1.0114555e+05,\n",
       "        4.0793454e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 1.5437241e+05, 1.2867185e+05,\n",
       "        3.8319962e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.3210734e+05, 8.1391770e+04,\n",
       "        4.6616842e+05]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "X_test_samp[:, 3] = labelencoder.fit_transform(X_test_samp[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X_test_samp = onehotencoder.fit_transform(X_test_samp).toarray()\n",
    "\n",
    "# Avoiding the Dummy Variable Trap\n",
    "X_test_samp = X_test_samp[:, 1:]\n",
    "X_test_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([212378.76779103, 201684.46921256, 178537.48221056, 180915.80126907,\n",
       "       163518.55037921])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = regressor.predict(X_test_samp)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Optimal Model using Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 1.0000000e+00, 1.6534920e+05, 1.3689780e+05,\n",
       "        4.7178410e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.6259770e+05, 1.5137759e+05,\n",
       "        4.4389853e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.5344151e+05, 1.0114555e+05,\n",
       "        4.0793454e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 1.4437241e+05, 1.1867185e+05,\n",
       "        3.8319962e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.4210734e+05, 9.1391770e+04,\n",
       "        3.6616842e+05]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Multiple Linear Regression : y=b0+b1x1+b2x2+......+bnxn\n",
    "## Add x0 which is linked with variable b0 and always equals to 1.\n",
    "#X=np.append(arr=X,values=np.ones((50,1)).astype(int), axis=1) ## Add 1; 50 times; axis=1 as it is a row\n",
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit\n",
    "y = dataset.iloc[:, 4].values  ## Grab Profit\n",
    "\n",
    "\n",
    "# Encoding categorical data : Country in this case\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray() ## to remove relationship\n",
    "\n",
    "# Avoiding the Dummy Variable Trap\n",
    "X = X[:, 1:] ## Remove the first column of X, ie, at index 0\n",
    "X[0:5]\n",
    "\n",
    "## y=bo+b1x1+bnxn can also be written as :\n",
    "## y=boxo+b1x1+bnxn [assuming x0=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.append(arr=np.ones((50,1)).astype(int),values=X, axis=1) ## axis=1 means add columns, 50=total number of records. \n",
    "#so it creates 50*1 matrix\n",
    "## np.ones=add 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
      "0  165349.20       136897.80        471784.10    New York  192261.83\n",
      "1  162597.70       151377.59        443898.53  California  191792.06\n",
      "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
      "3  144372.41       118671.85        383199.62    New York  182901.99\n",
      "4  142107.34        91391.77        366168.42     Florida  166187.94\n",
      "\n",
      "\n",
      "[[1.0000000e+00 0.0000000e+00 1.0000000e+00 1.6534920e+05 1.3689780e+05\n",
      "  4.7178410e+05]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6259770e+05 1.5137759e+05\n",
      "  4.4389853e+05]\n",
      " [1.0000000e+00 1.0000000e+00 0.0000000e+00 1.5344151e+05 1.0114555e+05\n",
      "  4.0793454e+05]\n",
      " [1.0000000e+00 0.0000000e+00 1.0000000e+00 1.4437241e+05 1.1867185e+05\n",
      "  3.8319962e+05]\n",
      " [1.0000000e+00 1.0000000e+00 0.0000000e+00 1.4210734e+05 9.1391770e+04\n",
      "  3.6616842e+05]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.head())\n",
    "print(\"\\n\")\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_Opt=X[:,[0,1,2,3,4,5]]\n",
    "#X_Opt=X[:,]\n",
    "#X_Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor_OLS=sm.OLS(endog=y,exog=X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   169.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 22 Dec 2018</td> <th>  Prob (F-statistic):</th> <td>1.34e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:22:33</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   1074.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.013e+04</td> <td> 6884.820</td> <td>    7.281</td> <td> 0.000</td> <td> 3.62e+04</td> <td>  6.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  198.7888</td> <td> 3371.007</td> <td>    0.059</td> <td> 0.953</td> <td>-6595.030</td> <td> 6992.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  -41.8870</td> <td> 3256.039</td> <td>   -0.013</td> <td> 0.990</td> <td>-6604.003</td> <td> 6520.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.369</td> <td> 0.000</td> <td>    0.712</td> <td>    0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.517</td> <td> 0.608</td> <td>   -0.132</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.574</td> <td> 0.123</td> <td>   -0.008</td> <td>    0.062</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.782</td> <th>  Durbin-Watson:     </th> <td>   1.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.41e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.572</td> <th>  Cond. No.          </th> <td>1.45e+06</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     169.9\n",
       "Date:                Sat, 22 Dec 2018   Prob (F-statistic):           1.34e-27\n",
       "Time:                        22:22:33   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1063.\n",
       "Df Residuals:                      44   BIC:                             1074.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\n",
       "x1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\n",
       "x2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\n",
       "x3             0.8060      0.046     17.369      0.000       0.712       0.900\n",
       "x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n",
       "x5             0.0270      0.017      1.574      0.123      -0.008       0.062\n",
       "==============================================================================\n",
       "Omnibus:                       14.782   Durbin-Watson:                   1.283\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\n",
       "Skew:                          -0.948   Prob(JB):                     2.41e-05\n",
       "Kurtosis:                       5.572   Cond. No.                     1.45e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.45e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ## Significance Level = 0.05 or 5%\n",
    "## Lower the P-Value , better it is\n",
    "## Lower P Value, reject the null hypothesuis, accept alternate hypothesis.\n",
    "## Alternate hypothesis= This particular variable is influencing the outcome\n",
    "## Find out the independent variable with highest P-Value\n",
    "## If the variable with highest P-value is greater than Significance level, drop that variable\n",
    "## Perform this exericse till the indepedent variable with highest P-Value is lower than significance level\n",
    "## x1=New York\n",
    "##x2=California\n",
    "## x3=R&D Spend\n",
    "## x4=Admin Spend\n",
    "##x5=Marketing Spend\n",
    "regressor_OLS.summary()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.000000e+00, 0.000000e+00, 1.653492e+05, 1.368978e+05,\n",
       "       4.717841e+05])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=X[:,[0,1,3,4,5]] ## REmoved x2=California, highest P-value\n",
    "X[0]\n",
    " \n",
    "\n",
    "##x1=New York\n",
    "## x2=R&D Spend\n",
    "## x3=Admin Spend\n",
    "##x4=Marketing Spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   217.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 22 Dec 2018</td> <th>  Prob (F-statistic):</th> <td>8.49e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:22:33</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1061.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th> <td>   1070.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.011e+04</td> <td> 6647.870</td> <td>    7.537</td> <td> 0.000</td> <td> 3.67e+04</td> <td> 6.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  220.1585</td> <td> 2900.536</td> <td>    0.076</td> <td> 0.940</td> <td>-5621.821</td> <td> 6062.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.606</td> <td> 0.000</td> <td>    0.714</td> <td>    0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.523</td> <td> 0.604</td> <td>   -0.131</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.592</td> <td> 0.118</td> <td>   -0.007</td> <td>    0.061</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.758</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.53e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.563</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.946\n",
       "Method:                 Least Squares   F-statistic:                     217.2\n",
       "Date:                Sat, 22 Dec 2018   Prob (F-statistic):           8.49e-29\n",
       "Time:                        22:22:33   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1061.\n",
       "Df Residuals:                      45   BIC:                             1070.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.011e+04   6647.870      7.537      0.000    3.67e+04    6.35e+04\n",
       "x1           220.1585   2900.536      0.076      0.940   -5621.821    6062.138\n",
       "x2             0.8060      0.046     17.606      0.000       0.714       0.898\n",
       "x3            -0.0270      0.052     -0.523      0.604      -0.131       0.077\n",
       "x4             0.0270      0.017      1.592      0.118      -0.007       0.061\n",
       "==============================================================================\n",
       "Omnibus:                       14.758   Durbin-Watson:                   1.282\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.172\n",
       "Skew:                          -0.948   Prob(JB):                     2.53e-05\n",
       "Kurtosis:                       5.563   Cond. No.                     1.40e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "regressor_OLS=sm.OLS(endog=y,exog=X).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   296.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 22 Dec 2018</td> <th>  Prob (F-statistic):</th> <td>4.53e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:22:34</td>     <th>  Log-Likelihood:    </th> <td> -525.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1066.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.012e+04</td> <td> 6572.353</td> <td>    7.626</td> <td> 0.000</td> <td> 3.69e+04</td> <td> 6.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8057</td> <td>    0.045</td> <td>   17.846</td> <td> 0.000</td> <td>    0.715</td> <td>    0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0268</td> <td>    0.051</td> <td>   -0.526</td> <td> 0.602</td> <td>   -0.130</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0272</td> <td>    0.016</td> <td>    1.655</td> <td> 0.105</td> <td>   -0.006</td> <td>    0.060</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.838</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.949</td> <th>  Prob(JB):          </th> <td>2.21e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.586</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.948\n",
       "Method:                 Least Squares   F-statistic:                     296.0\n",
       "Date:                Sat, 22 Dec 2018   Prob (F-statistic):           4.53e-30\n",
       "Time:                        22:22:34   Log-Likelihood:                -525.39\n",
       "No. Observations:                  50   AIC:                             1059.\n",
       "Df Residuals:                      46   BIC:                             1066.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\n",
       "x1             0.8057      0.045     17.846      0.000       0.715       0.897\n",
       "x2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\n",
       "x3             0.0272      0.016      1.655      0.105      -0.006       0.060\n",
       "==============================================================================\n",
       "Omnibus:                       14.838   Durbin-Watson:                   1.282\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\n",
       "Skew:                          -0.949   Prob(JB):                     2.21e-05\n",
       "Kurtosis:                       5.586   Cond. No.                     1.40e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=X[:,[0,2,3,4]] ## REmoved x1=New York, highest P-value.\n",
    "regressor_OLS=sm.OLS(endog=y,exog=X).fit()\n",
    "regressor_OLS.summary() \n",
    "\n",
    "## x1=R&D Spend\n",
    "## x2=Admin Spend\n",
    "## x3=Marketing Spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   450.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 22 Dec 2018</td> <th>  Prob (F-statistic):</th> <td>2.16e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:22:34</td>     <th>  Log-Likelihood:    </th> <td> -525.54</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1057.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    47</td>      <th>  BIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.698e+04</td> <td> 2689.933</td> <td>   17.464</td> <td> 0.000</td> <td> 4.16e+04</td> <td> 5.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.7966</td> <td>    0.041</td> <td>   19.266</td> <td> 0.000</td> <td>    0.713</td> <td>    0.880</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0299</td> <td>    0.016</td> <td>    1.927</td> <td> 0.060</td> <td>   -0.001</td> <td>    0.061</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.677</td> <th>  Durbin-Watson:     </th> <td>   1.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.939</td> <th>  Prob(JB):          </th> <td>2.54e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.575</td> <th>  Cond. No.          </th> <td>5.32e+05</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.950\n",
       "Model:                            OLS   Adj. R-squared:                  0.948\n",
       "Method:                 Least Squares   F-statistic:                     450.8\n",
       "Date:                Sat, 22 Dec 2018   Prob (F-statistic):           2.16e-31\n",
       "Time:                        22:22:34   Log-Likelihood:                -525.54\n",
       "No. Observations:                  50   AIC:                             1057.\n",
       "Df Residuals:                      47   BIC:                             1063.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n",
       "x1             0.7966      0.041     19.266      0.000       0.713       0.880\n",
       "x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n",
       "==============================================================================\n",
       "Omnibus:                       14.677   Durbin-Watson:                   1.257\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n",
       "Skew:                          -0.939   Prob(JB):                     2.54e-05\n",
       "Kurtosis:                       5.575   Cond. No.                     5.32e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.32e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=X[:,[0,1,3]] ## Removed x2=Admin Spend, highest P-value; (3 and 5: R&D Spend and Marketing Spend)\n",
    "regressor_OLS=sm.OLS(endog=y,exog=X).fit()\n",
    "regressor_OLS.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   849.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 22 Dec 2018</td> <th>  Prob (F-statistic):</th> <td>3.50e-32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:22:34</td>     <th>  Log-Likelihood:    </th> <td> -527.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.903e+04</td> <td> 2537.897</td> <td>   19.320</td> <td> 0.000</td> <td> 4.39e+04</td> <td> 5.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.8543</td> <td>    0.029</td> <td>   29.151</td> <td> 0.000</td> <td>    0.795</td> <td>    0.913</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.727</td> <th>  Durbin-Watson:     </th> <td>   1.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  18.536</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.911</td> <th>  Prob(JB):          </th> <td>9.44e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.361</td> <th>  Cond. No.          </th> <td>1.65e+05</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.947\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     849.8\n",
       "Date:                Sat, 22 Dec 2018   Prob (F-statistic):           3.50e-32\n",
       "Time:                        22:22:34   Log-Likelihood:                -527.44\n",
       "No. Observations:                  50   AIC:                             1059.\n",
       "Df Residuals:                      48   BIC:                             1063.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\n",
       "x1             0.8543      0.029     29.151      0.000       0.795       0.913\n",
       "==============================================================================\n",
       "Omnibus:                       13.727   Durbin-Watson:                   1.116\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\n",
       "Skew:                          -0.911   Prob(JB):                     9.44e-05\n",
       "Kurtosis:                       5.361   Cond. No.                     1.65e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.65e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## x1=R&D Spend\n",
    "## x2=Marketing Spend\n",
    "X=X[:,[0,1]] ## Removed x2=Marketing Spend, highest P-value; (Left Over; x2: R&D Spend )\n",
    "regressor_OLS=sm.OLS(endog=y,exog=X).fit()\n",
    "regressor_OLS.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
      "0  165349.20       136897.80        471784.10    New York  192261.83\n",
      "1  162597.70       151377.59        443898.53  California  191792.06\n",
      "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
      "3  144372.41       118671.85        383199.62    New York  182901.99\n",
      "4  142107.34        91391.77        366168.42     Florida  166187.94\n",
      "\n",
      "\n",
      "[1.000000e+00 1.653492e+05]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.head())\n",
    "print(\"\\n\")\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000000e+00 5.549395e+04]\n",
      " [1.000000e+00 4.601402e+04]\n",
      " [1.000000e+00 7.532887e+04]\n",
      " [1.000000e+00 4.642607e+04]\n",
      " [1.000000e+00 9.174916e+04]]\n",
      "[ 96778.92  96479.51 105733.54  96712.8  124266.9 ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:5])\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104667.27805998 134150.83410578 135207.80019517  72170.54428856\n",
      " 179090.58602508 109824.77386586  65644.27773757 100481.43277139\n",
      " 111431.75202432 169438.14843539]\n",
      "\n",
      "\n",
      "[103282.38 144259.4  146121.95  77798.83 191050.39 105008.31  81229.06\n",
      "  97483.56 110352.25 166187.94]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(y_pred)\n",
    "print(\"\\n\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 6772.453280477903\n",
      "MSE: 68473440.7190593\n",
      "RMSE: 8274.868018225989\n",
      "Explained Variance Score: 95.93900820926665\n",
      "Cross val Mean Abs Error: -7601.576216809316\n",
      "Cross Val Explained Variance: 66.3524825988975\n",
      "r2: 0.946458760778722\n",
      "adj_r2= 0.9311612638583568\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "## Higher the Explained Variance Score, the better the model is:\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_test, y_pred)*100)\n",
    "## 95.9% of the variance is explained by the model\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='neg_mean_absolute_error') \n",
    "print (\"Cross val Mean Abs Error:\",regression_avg.mean())\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='explained_variance') \n",
    "print (\"Cross Val Explained Variance:\",regression_avg.mean()*100)\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score \n",
    "print('r2:',r2_score(y_test, y_pred)) ## Closer to 1 means better prediction\n",
    "\n",
    "adj_r2=1 - float(len(y_test)-1)/(len(y_test)-len(regressor.coef_)-1)*(1 - metrics.r2_score(y_test,y_pred))\n",
    "print(\"adj_r2=\",adj_r2) ##Closer to 1 the better the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Including Marketing Spend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
      "0  165349.20       136897.80        471784.10    New York  192261.83\n",
      "1  162597.70       151377.59        443898.53  California  191792.06\n",
      "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
      "3  144372.41       118671.85        383199.62    New York  182901.99\n",
      "4  142107.34        91391.77        366168.42     Florida  166187.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([165349.2, 471784.1], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fitting the model with training data of significant independent variable\n",
    "print(dataset.head())\n",
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit\n",
    "X=X[:,[0,2]] ## Grabbing R&D Spend and Marketing Spend\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([102284.64605183, 133873.92383812, 134182.1495165 ,  73701.1069363 ,\n",
       "       180642.25299736, 114717.24903894,  68335.07575312,  97433.45922275,\n",
       "       114580.92136452, 170343.31979498])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 6886.594588246507\n",
      "MSE: 67220275.37568115\n",
      "RMSE: 8198.797190788484\n",
      "Explained Variance Score: 95.57894709421504\n",
      "Cross Val Mean Absolute Error -7269.198451538758\n",
      "Cross Val Explained Variance: 77.25927621238642\n",
      "r2: 0.9474386447268489\n",
      "adj_r2= 0.9324211146488057\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "## Higher the Explained Variance Score, the better the model is:\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_test, y_pred)*100)\n",
    "## 95.9% of the variance is explained by the model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='neg_mean_absolute_error') \n",
    "print (\"Cross Val Mean Absolute Error\",regression_avg.mean())\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='explained_variance') \n",
    "print (\"Cross Val Explained Variance:\",regression_avg.mean()*100)\n",
    "\n",
    "from sklearn.metrics import r2_score \n",
    "print('r2:',r2_score(y_test, y_pred)) ## Closer to 1 means better prediction\n",
    "\n",
    "adj_r2=1 - float(len(y_test)-1)/(len(y_test)-len(regressor.coef_)-1)*(1 - metrics.r2_score(y_test,y_pred))\n",
    "print(\"adj_r2=\",adj_r2) ##Closer to 1 the better the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to automate Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165349.2, 136897.8, 471784.1, 'New York'],\n",
       "       [162597.7, 151377.59, 443898.53, 'California'],\n",
       "       [153441.51, 101145.55, 407934.54, 'Florida'],\n",
       "       [144372.41, 118671.85, 383199.62, 'New York'],\n",
       "       [142107.34, 91391.77, 366168.42, 'Florida']], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit\n",
    "y = dataset.iloc[:, 4].values  ## Grab Profit\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.6534920e+05,\n",
       "        1.3689780e+05, 4.7178410e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.6259770e+05,\n",
       "        1.5137759e+05, 4.4389853e+05],\n",
       "       [1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.5344151e+05,\n",
       "        1.0114555e+05, 4.0793454e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.4437241e+05,\n",
       "        1.1867185e+05, 3.8319962e+05],\n",
       "       [1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.4210734e+05,\n",
       "        9.1391770e+04, 3.6616842e+05]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Multiple Linear Regression : y=b0+b1x1+b2x2+......+bnxn\n",
    "## Add x0 which is linked with variable b0 and always equals to 1.\n",
    "#X=np.append(arr=X,values=np.ones((50,1)).astype(int), axis=1) ## Add 1; 50 times; axis=1 as it is a row\n",
    "\n",
    "# Encoding categorical data : Country in this case\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray() ## to remove relationship\n",
    "\n",
    "# Avoiding the Dummy Variable Trap\n",
    "X = X[:, 1:] ## Remove the first column of X, ie, at index 0\n",
    "X[0:5]\n",
    "\n",
    "X=np.append(arr=np.ones((50,1)).astype(int),values=X, axis=1)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        import statsmodels.formula.api as sm\n",
    "        def backwardElimination(x, sl):\n",
    "            numVars = len(x[0])\n",
    "            for i in range(0, numVars):\n",
    "                regressor_OLS = sm.OLS(y, x).fit()\n",
    "                maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "                if maxVar > sl:\n",
    "                    for j in range(0, numVars - i):\n",
    "                        if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                            x = np.delete(x, j, 1)\n",
    "            regressor_OLS.summary()\n",
    "            return x\n",
    "         \n",
    "        SL = 0.05\n",
    "        X = backwardElimination(X, SL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
      "0  165349.20       136897.80        471784.10    New York  192261.83\n",
      "1  162597.70       151377.59        443898.53  California  191792.06\n",
      "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
      "3  144372.41       118671.85        383199.62    New York  182901.99\n",
      "4  142107.34        91391.77        366168.42     Florida  166187.94\n",
      "\n",
      "\n",
      "[1.000000e+00 1.653492e+05]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.head())\n",
    "print(\"\\n\")\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104667.27805998, 134150.83410578, 135207.80019517,  72170.54428856,\n",
       "       179090.58602508, 109824.77386586,  65644.27773757, 100481.43277139,\n",
       "       111431.75202432, 169438.14843539])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X=X[:,[1]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 6772.453280477899\n",
      "MSE: 68473440.71905932\n",
      "RMSE: 8274.86801822599\n",
      "Explained Variance Score: 95.93900820926665\n",
      "Cross Val Mean Absolute Error -7601.576216809315\n",
      "Cross Val Explained Variance: 66.3524825988975\n",
      "r2: 0.9464587607787219\n",
      "adj_r2= 0.9397661058760621\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "## Higher the Explained Variance Score, the better the model is:\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_test, y_pred)*100)\n",
    "## 95.9% of the variance is explained by the model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='neg_mean_absolute_error') \n",
    "print (\"Cross Val Mean Absolute Error\",regression_avg.mean())\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='explained_variance') \n",
    "print (\"Cross Val Explained Variance:\",regression_avg.mean()*100)\n",
    "\n",
    "from sklearn.metrics import r2_score \n",
    "print('r2:',r2_score(y_test, y_pred)) ## Closer to 1 means better prediction\n",
    "\n",
    "adj_r2=1 - float(len(y_test)-1)/(len(y_test)-len(regressor.coef_)-1)*(1 - metrics.r2_score(y_test,y_pred))\n",
    "print(\"adj_r2=\",adj_r2) ##Closer to 1 the better the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Automate Backward Elimination with R-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165349.2, 136897.8, 471784.1, 'New York'],\n",
       "       [162597.7, 151377.59, 443898.53, 'California'],\n",
       "       [153441.51, 101145.55, 407934.54, 'Florida'],\n",
       "       [144372.41, 118671.85, 383199.62, 'New York'],\n",
       "       [142107.34, 91391.77, 366168.42, 'Florida']], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit\n",
    "y = dataset.iloc[:, 4].values  ## Grab Profit\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.6534920e+05,\n",
       "        1.3689780e+05, 4.7178410e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.6259770e+05,\n",
       "        1.5137759e+05, 4.4389853e+05],\n",
       "       [1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.5344151e+05,\n",
       "        1.0114555e+05, 4.0793454e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.4437241e+05,\n",
       "        1.1867185e+05, 3.8319962e+05],\n",
       "       [1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.4210734e+05,\n",
       "        9.1391770e+04, 3.6616842e+05]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Multiple Linear Regression : y=b0+b1x1+b2x2+......+bnxn\n",
    "## Add x0 which is linked with variable b0 and always equals to 1.\n",
    "#X=np.append(arr=X,values=np.ones((50,1)).astype(int), axis=1) ## Add 1; 50 times; axis=1 as it is a row\n",
    "X = dataset.iloc[:, :-1].values ## Grab everything except Profit\n",
    "y = dataset.iloc[:, 4].values  ## Grab Profit\n",
    "\n",
    "\n",
    "# Encoding categorical data : Country in this case\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray() ## to remove relationship\n",
    "\n",
    "# Avoiding the Dummy Variable Trap\n",
    "X = X[:, 1:] ## Remove the first column of X, ie, at index 0\n",
    "X[0:5]\n",
    "\n",
    "X=np.append(arr=np.ones((50,1)).astype(int),values=X, axis=1)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ## Though significance level = 0.05, however, if eliminating the variable is reducing the adjusted r2, then we are retaining \n",
    "    ## the variable\n",
    "    def backwardElimination(x, SL):\n",
    "            numVars = len(x[0])\n",
    "            temp = np.zeros((50,6)).astype(int) ## Only change this line\n",
    "            for i in range(0, numVars):\n",
    "                regressor_OLS = sm.OLS(y, x).fit()\n",
    "                maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "                adjR_before = regressor_OLS.rsquared_adj.astype(float)\n",
    "                if maxVar > SL:\n",
    "                    for j in range(0, numVars - i):\n",
    "                        if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                            temp[:,j] = x[:, j]\n",
    "                            x = np.delete(x, j, 1)\n",
    "                            tmp_regressor = sm.OLS(y, x).fit()\n",
    "                            adjR_after = tmp_regressor.rsquared_adj.astype(float)\n",
    "                            if (adjR_before >= adjR_after):\n",
    "                                x_rollback = np.hstack((x, temp[:,[0,j]]))\n",
    "                                x_rollback = np.delete(x_rollback, j, 1)\n",
    "                                print (regressor_OLS.summary())\n",
    "                                return x_rollback\n",
    "                            else:\n",
    "                                continue\n",
    "            regressor_OLS.summary()\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.950\n",
      "Model:                            OLS   Adj. R-squared:                  0.948\n",
      "Method:                 Least Squares   F-statistic:                     450.8\n",
      "Date:                Sat, 22 Dec 2018   Prob (F-statistic):           2.16e-31\n",
      "Time:                        22:24:13   Log-Likelihood:                -525.54\n",
      "No. Observations:                  50   AIC:                             1057.\n",
      "Df Residuals:                      47   BIC:                             1063.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\n",
      "x1             0.7966      0.041     19.266      0.000       0.713       0.880\n",
      "x2             0.0299      0.016      1.927      0.060      -0.001       0.061\n",
      "==============================================================================\n",
      "Omnibus:                       14.677   Durbin-Watson:                   1.257\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\n",
      "Skew:                          -0.939   Prob(JB):                     2.54e-05\n",
      "Kurtosis:                       5.575   Cond. No.                     5.32e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.32e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "SL = 0.05\n",
    "X = backwardElimination(X, SL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
      "0  165349.20       136897.80        471784.10    New York  192261.83\n",
      "1  162597.70       151377.59        443898.53  California  191792.06\n",
      "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
      "3  144372.41       118671.85        383199.62    New York  182901.99\n",
      "4  142107.34        91391.77        366168.42     Florida  166187.94\n",
      "\n",
      "\n",
      "[1.000000e+00 1.653492e+05 4.717840e+05]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.head())\n",
    "print(\"\\n\")\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([102284.66001734, 133873.92217929, 134182.13671038,  73701.09355224,\n",
       "       180642.24863739, 114717.22534401,  68335.07916061,  97433.47371492,\n",
       "       114580.91994341, 170343.32062073])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X=X[:,[1,2]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 6886.592193598828\n",
      "MSE: 67220271.20117109\n",
      "RMSE: 8198.796936207842\n",
      "Explained Variance Score: 95.57894867960009\n",
      "Cross Val Mean Absolute Error -7269.1990129464075\n",
      "Cross Val Explained Variance: 77.25921592525557\n",
      "r2: 0.9474386479910114\n",
      "adj_r2= 0.9324211188455861\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "## Higher the Explained Variance Score, the better the model is:\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_test, y_pred)*100)\n",
    "## 95.9% of the variance is explained by the model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='neg_mean_absolute_error') \n",
    "print (\"Cross Val Mean Absolute Error\",regression_avg.mean())\n",
    "\n",
    "from sklearn.model_selection import cross_val_score \n",
    "regression_avg = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10,scoring='explained_variance') \n",
    "print (\"Cross Val Explained Variance:\",regression_avg.mean()*100)\n",
    "\n",
    "from sklearn.metrics import r2_score \n",
    "print('r2:',r2_score(y_test, y_pred)) ## Closer to 1 means better prediction\n",
    "\n",
    "adj_r2=1 - float(len(y_test)-1)/(len(y_test)-len(regressor.coef_)-1)*(1 - metrics.r2_score(y_test,y_pred))\n",
    "print(\"adj_r2=\",adj_r2) ##Closer to 1 the better the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
